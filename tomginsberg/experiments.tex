Our experiments are carried out on natural distribution shifts across multiple domains, modalities, and model types.
We use the \textit{CIFAR-10.1} dataset~\citep{cifar101} where shift comes from subtle changes in the dataset creation processes,
the \textit{Camelyon17 dataset}~\citep{camelyon} for metastases detection in histopathological slides from multiple hospitals, as well as the \textit{UCI heart disease} dataset~\citep{misc_heart_disease_45} which contains tabular features collected across international health systems and indicators of heart disease.
We present unseen source an target domain performance of base models trained on source data in Appendix \autoref{tab:datasets} which shows significant performance drops as an indicator for the hamrfulness of these shifts.
See \autoref{sec:data} for more details on datasets.

\xhdr{Learning Constrained Disagreement}
We begin by training ensembles of $10$ CDCs using the \textit{disagreement cross entropy} (DCE) with CIFAR-10 as $\boldP$ and CIFAR-10.1 as $\boldQ$ for 100 random runs at a sample size of $50$ (see \autoref{sec:expdet} for training details).
The results in \autoref{fig:disrates} empirically validates minimizng the DCE as a CDC learning objective.
The first observation is that when an unseen set is drawn from a shifted distribution $\Qc$, the empirical disagreement rate $\phi_\boldQ$ grows significantly larger
than the baseline disagreement rate $\phi_\boldP$.
Next, we see that CDCs preserve accuracy on data from the training distribution.
Finally, as the ensemble size increases (and disagreed upon points are removed) we see that the accuracy of the classifier increases.
This indicates that the points that are disagreed upon early on in the algorithm are those that would have been misclassified.

\begin{figure}[!htb]
    \vspace{-3mm}
    \centering
    \includegraphics[width=\linewidth]{images/acc_dis.pdf}
    \vspace{-7mm}
    \caption{\small \textbf{Ensemble Size vs Properties of Constrained disagreement classifiers on CIFAR-10/10.1:}
    (Left) We see that for all ensemble sizes, there is lower disagreement on unshifted data (CIFAR-10) compared to disagreement on shifted data (CIFAR-10.1). (Center) Constrained disagreement does not compromise in-distribution performance.
        (Right) As the ensemble grows the selective classification accuracy computed on the set of test examples that all models agree on increases both on in-distribution and out-of-distribution data.
        Confidence intervals are computed as $\pm$ one standard deviation across experiments.}
    \label{fig:disrates}
    \vspace{-1mm}
\end{figure}

\xhdr{Shift Detection Setup} We evaluate the \method\ in a standard two-sample testing scenario similar to prior work~\citep{zhao2022comparing}.
Given two datasets $\boldP = \{x_1,\dots,x_N\}$ (drawn from $\Pc$) and $\boldQ = \{\hat{x}_1,\dots,\hat{x}_n\}$ (drawn from $\Qc$) and classifier $f$,
we seek to rule out the null hypothesis ($\Pc = \Qc$) at the $5\%$ significance level.
To guarantee fixed significance we employ a permutation test by first sampling from the distribution of $p$-values
derived by the \method\ where the null hypothesis $\Pc=\Qc$ holds (i.e., $\boldQ$ is drawn $\Pc$).
We then compute a threshold over the observed set of $p$-values that sets the false positive rate to $5\%$.
This step can be performed in advance of deployment as it only requires access to $\boldP$.
To mimic deployment settings where we wish to identify covariate shift quickly,
we assume access to far more samples from $\Pc$ compared to $\Qc$.
For each dataset, we begin by training a base classifier on the unshifted dataset.
We evaluate the detection of covariate shift on 100 randomly selected test sets of $n=$ 10, 20 and 50 samples from $\Qc$.
In all cases we train a maximum ensemble size of 5 (parameter $\aleph$ in \autoref{alg:detectron}).
To prevent CDCs from overfitting in the case of small test set sizes, we perform early stopping if in-distribution validation performance drops by over 5\% from the measured performance of the base classifier. Hyperparameters and training details for all models can be found in \autoref{sec:expdet}.

\xhdr{Evaluation} We report the \textit{True Positive Rate at 5\% Significance Level (TPR@5)} aggregated over $100$ randomly selected sets $\boldQ$.
This signifies how often our method correctly identifies covariate shift ($\Pc\neq\Qc$) while only incorrectly identifying shift 5\% of the time.
This is also referred to as the statistical power of a test where the significance level or probability of making a type I error is 5\%.
% (2) \textit{Area Under the Receiver Operating Characteristic Curve (AUC)}: To showcase the sensitivity of our method - a desirable characteristic in high-risk domains where false negatives are far more costly than false positives - we report the area under the true positive vs false positive curve generated by varying the significance level from 0 to 1. %This is identical to the conventional interpretation of the AUC for classification problems.

\xhdr{Baselines} We compare the \method\ against several methods for OOD detection, uncertainty estimation and covariate shift detection. \textit{Deep Ensembles}~\cite{trustuncert} using both (1) \textit{disagreement} and (2) \textit{entropy} scoring methods as a direct ablation to the CDC approach (3) \textit{Black Box Shift Detection (BBSD)}~\citep{bbsd}.
(4) \textit{Relative Mahalanobis Distance (RMD)}~\citep{relmahala}.
(5) \textit{Classifier Two Sample Test (CTST)}~\citep{paz2017revisiting}.
(6) \textit{Deep Kernel MMD (MMD-D)}~\citep{liu2020learning}.
(7) \textit{H Divergence (H-Div)}~\citep{zhao2022comparing}.
For more information on baselines see Appendix \autoref{subsec:baselines}.

\xhdr{Shift Detection Experiments}
We begin with an analysis of the performance of the \method\ on the UCI Heart Disease dataset.
Using a sample size ranging from 10 to 100 we compute the TPR@5 averaged over 100 trials and plot the results in \autoref{fig:uci_plot}.
As the \method\ is model agnostic we use gradient boosted trees (XGBoost~\citep{xgb})
for \method\ and CTST methods while the remaining baselines use a 2 layer MLP that achieves similar in-distribution performance.
Further results for sample sizes of 10, 20 and 50 on all datasets are shown in \autoref{tab:results}.
We report the mean and standard error of TPR@5 computed on 100 samples for each trial.

\begin{figure}[!htb]
    \vspace{-10mm}
    \centering
    \includegraphics[width=\textwidth]{images/uci_plot.pdf}
    \vspace{-5mm}
    \caption{\small \textbf{True positive rate at the 5\% significance level} for the \method\ and baseline methods for detection of covariate shift on the UCI heart disease dataset.
    The \method\ (Entropy) is shown to uniformly outperform baselines.
    Confidence intervals are excluded for visual clarity but are found in \autoref{tab:results}.}
    \label{fig:uci_plot}
\end{figure}

\input{results}