Covariate shift is the tendency for a covariate distribution at test time $p_{\text{test}}(x)$ to differ from that seen
during training $p_{\text{train}}(x)$ while the underlying prediction concept $y$ remains fixed~\citep{covbook}
\[p_{\text{train}}(x)\neq p_{\text{test}}(x) \text{ while } p_{\text{train}}(y|x)=p_{\text{test}}(y|x)\]
The problem of detecting variate shift is important at a fundamental level in machine learning.
From a classical learning theory perspective, predictive ML algorithms that operate on unseen data
can only claim reasonable performance guarantees when such data is exchangeable with the labeled samples which they were trained and tested on~\citep{Haussler90probablyapproximately}.
The violation of this assumption in general is known as distribution shift.
However, in a typical deployment setting where only covariates are observed one cannot detect changes in $p(y)$ or $p(y|x)$ leaving covariate shift as the only remaining phenomenon identifiable from data.
The related problems of \textit{label shift} ({shift in $p(y)$}) and \textit{concept shift} (shift in $p(y|x)$) pose an additional problems for maintaining the robustness of deployed ML systems, but are not the topic of this work.
To understand contemporary methods for detecting covariate shift in the high dimensional data that is typical of the covariates found in real-world ML systems we must first take a segway back to the underlying concepts in probability and statistics.


\section{Two Sample Statistical Testing}
\textit{If the reader is familiar with the subject of two sample testing they may continue to \autoref{sec:cov-high-dimensions}.}
\smallbreak\noindent
The problem of detecting when two distributions $\Pc$ and $\Qc$ over $\mathcal{X}$ are different given only finite samples $\boldP=\{X_1,\ldots X_n\}\overset{\text{iid}}{\sim} {\Pc}$ and $\boldQ=\{\tilde{X}_1,\ldots \tilde{X}_m\}\overset{\text{iid}}{\sim} {\Qc}$ is one of the most fundamental problems in statistics.
The general problems seeks to rule out a \textit{null hypothesis} $\mathcal{H}_0:\ \Pc=\Qc$ in favor of an alternative $\mathcal{H_a}:\ \Pc\neq \Qc$ at a given significance level $\alpha$.
The significance level is equivalent to the false positive rate or probability of making a type I error, which fixes a threshold for the test such that when $\Pc=\Qc$, $\mathcal{H}_0$ is ruled out with probability no more than $\alpha$.
The power of a statistical test, often denoted as $1-\beta$ where $\beta$ is the probability of making a type II error, is the probability that the test correctly rules out $\mathcal{H}_0$ while still maintaining a significance level of $\alpha$.
While less standard in statistics literature, it is natural to measure the area under the receiver operating characteristics (AUROC) as area under curve $1-\beta$ as a function of $\alpha$ for $\alpha\in [0,1]$.
For a more rigours mathematical underpinning of two sample testing see section 2.1 in ~\citeauthor{twosampletestingbackground}.
In the following subsections we highlight several important approaches to two sample testing that will be used in much of the later methodologies.

\smallbreak
\subsection*{Kolmogorov–Smirnov test} An elegant and robust two sample test that requires no assumptions on the distributions $\Pc$ and $\Qc$ besides $\mathcal{X}=\mathbb{R}$ is the {Kolmogorov–Smirnov test} (KS test)~\citep{kolmogorov1933sulla}.
The KS test leverages the result of the Glivenko–Cantelli theorem~\citep{glivenko1933sulla} to construct a test statistic defined by the largest absolute difference between the two empirical cumulative distribution functions across all observed values.
As the Glivenko–Cantelli theorem is simple yet fundamental to understanding the KS test we present it in full below:
\begin{theorem}
    [Glivenko–Cantelli~\citep{glivenko1933sulla}]
    Assume that $X_1,X_2,\dots$ are independent and identically-distributed random variables in $\mathbb {R}$  with common cumulative distribution function $F(x)$.
    The empirical distribution function for $X_{1},\dots ,X_{n}$ is defined by
    \begin{equation}
        F_n(x)=\frac{1}{n} \sum_{i=1}^n I_{\left[X_i, \infty\right)}(x)=\frac{1}{n}\left|\left\{1 \leq i \leq n \mid X_i \leq x\right\}\right|\label{eq:empdist}
    \end{equation}
    where $I_{C}$ is the indicator function of the set $C$.
    \noindent
    For every (fixed) $x$, $F_n(x)$ is a sequence of random variables which converge to $F(x)$ almost surely by the strong law of large numbers.
    \begin{equation}
        \left\|F_n-F\right\|_{\infty}=\sup _{x \in \mathbb{R}}\left|F_n(x)-F(x)\right| \longrightarrow 0 \text { almost surely }
    \end{equation}
\end{theorem}

To perform a KS two sample test, one computes the exact probability under the null hypothesis $\Pc=\Qc$ of obtaining a rarer value compared to the observation of the test statistic
\[D_{\boldP,\boldQ}\triangleq \sup_{x \in \mathbb{R}}\left|F_{\boldP}(x)-F_{\boldQ}(x)\right|\]
Where $F_{\mathbf{S}}$ is the empirical CDF for a set of samples $\mathbf{S}$.

Computing the probability of observing a rarer test statistic is non-trivial and in general maps to the problem of counting the number of distinct paths that pass outside the specified diagonal on a $|\boldP| \times |\boldQ|$ rectangular grid~\citep{drew2000computing}.
However, for large $\boldP$ and $\boldQ$ asymptotic approximations exist that can greatly reduce the computational complexity.
A shortcoming of the KS test is that it sacrifices statistical power in order to be more flexible to handle arbitrary differences between distributions.

\subsection*{Binomial test} While the KS test present a general approach to univariate two sample testing, we can often use partial knowledge over the distribution that generates our samples to design of a more powerful test;
the binomial test is one such example.
The test considers a binomially distributed random variable $X$ with unknown rate $q$ i.e., $X\sim \text{Binomial}(n, q)$ for which we observe a single sample $x$ (hence $\mathcal{X}=\{0,\ldots, n\}$).
Since the binomial distribution is defined as a sum of i.i.d.\ Bernoulli random variables with the same rate, $x$ may equivalently be interpreted as a set of $n$ samples of which $x$ are $1$ and $n-x$ are $0$.
We wish to either confirm or rule out that $x$ was sampled from a baseline binomial distribution with known rate $p$.
We do this by computing the probability of observing an event at least as rare as $X=x$ under the null hypothesis that $p=q$.
This quantity can be computed exactly using the symmetry of the binomial distribution.
\begin{align*}
    \mathbb{P}_{X\sim \text{Bin}(n, p)}(X \text{ as least as rare as } x) &= 2\times \mathbb{P}_{X\sim \text{Bin}(n, p)}(X \geq x)\\
    &= 2 \sum_{k=x}^{n} \mathbb{P}[X=k] \\
    &= 2 \sum _{k=x}^n (1-p)^{n-k} p^k \binom{n}{k} = 2\frac{B_p(x,n-x+1)}{B(x,n-x+1)}
\end{align*}
Where $B_z(\alpha, \beta)$ is the incomplete Beta function and $B(\alpha, \beta)$ is the beta function.
By the convenient construction of the test one can query the likelihood specifically for one-sided alternatives;
something that is more practical when there is specific knowledge related to the alternative hypothesis.
For example, one can test if a coin is unfairly biased towards heads given an observation of $60/100$ heads, in this case the one-sided binomial test admits a $p$-value of $\approx 0.028$ suggesting that we can rule out at the $5\%$ level that the coin is fair
in favor of it being biased towards heads.
\smallbreak
Do to the simple structure of the binomial test one can derive a simple summation formula for computing the
exact statistical power as a function of $n$, $p$, $q$ and the significance level $\alpha$.
This quantity corresponds to the probability of observing a $p$-value of less than or equal to $\alpha$ when running a
binomial test on sample from a random variable $X\sim \text{Bin}(n,q)$ against a baseline rate $p$.
\begin{align*}
    \mathbb{P}_{Y\sim \text{Bin}(n, q)} \left( \mathbb{P}_{X\sim \text{Bin}(n, p)}(X \geq Y) \leq \frac{\alpha}{2} \right)&= \mathbb{P}_{Y\sim \text{Bin}(n, q)}\left(  \sum_{k=k}^n \binom{n}{k} (1-p)^{n-k} p^k  \leq \frac{\alpha}{2} \right)\\
    &= \mathbb{E}_{Y\sim \text{Bin}(n, q)}\left(  \mathbb{I}\left[\sum_{k=k}^n \binom{n}{k} (1-p)^{n-k} p^k  \leq \frac{\alpha}{2}\right] \right)\\
    &= \sum_{y=0}^n \binom{n}{y} \mathbb{I}\left[  \sum_{k=y}^n \binom{n}{k} (1-p)^{n-k} p^k \leq \frac{\alpha}{2} \right] q^y (1-q)^{n-y}
\end{align*}
We use $\mathbb{I}[\cdot]$ as a binary indicator.
We verify this formula using a simple monte carlo simulation with $p=.5$, $q=.6$, $n=100$ and $\alpha=0.05$.
The statistical power and standard error estimated over $10,000$ draws gives $0.461\pm 0.005$ whereas the above summation yields a value of $0.4621$.
One important note is that under this simple construction the binomial test is not a two sample test, but merely a single sample test.
However, in a setting where one has access to many more sample in $\boldP$ compared to that in $\boldQ$
(which will be the primary setting of the upcoming methodology), one can reasonably estimate the baseline success rate $p$ from $\boldP$ and use the binomial test as is.
In other settings where the assumption of $|\boldP|\gg|\boldQ|$ does not hold, one may resort to Barnard's exact test~\citep{Barnardp88:online} or the often more powerful alternative Fisher's exact test~\citep{fisher_r_a_1922_1449484}.

\subsection*{Integral Probability Metrics \& Maximum Mean Discrepancy}
A general method to define a measure between probability distributions is via an integral probability metric or IPM~\citep{IPMS}.
An IPM is defined as the supremum over a set of functions $F$ on the absolute difference between the expectations under $\Pc$ and $\Qc$.
\begin{equation*}
    \text{IPM}_F(P \| Q)=\sup_{f\in F}|\mathbb{E}_{x\sim P}\left[ f(x)\right] - \mathbb{E}_{x\sim Q}\left[ f(x)\right]|
\end{equation*}
When $F$ is chosen as the set of all functions with bounded $\infty-$norm $F = \left\{f:\|f\|_{\infty} \leq 1\right\}$
the associated IPM results in the well known total variation distance~\citep{ipm-divergences}.

The Maximum Mean Discrepancy (MMD) is formalized as an IPM given $F$ to be the n Reproducing Kernel Hilbert Space $\mathcal{H}_k$ with a kernel $k: \mathcal{X}\times \mathcal{X} \to \mathbb{R}$~\citep{twosampletestingbackground}.
\begin{equation*}
    \operatorname{MMD}\left(\Pc, \Qc ; \mathcal{H}_k\right) = \sup _{f \in \mathcal{H}_k:\|f\|_{\mathcal{H}_k} \leq 1}\left|\mathbb{E}_{X \sim \Pc}[f(X)]-\mathbb{E}_{Y \sim \Qc}[f(Y)]\right|
\end{equation*}
When we chose a characteristic kernel such that $\operatorname{MMD}\left(\Pc, \Qc ; \mathcal{H}_k\right)=0\iff \Pc=\Qc$ \citeauthor{gretton2012kernel} construct a quadratic time MMD estimator using samples $\boldP$ and $\boldQ$ of sizes $n$ and $m$ respectively:
\begin{equation}
    \begin{aligned}
        \widehat{\mathrm{MMD}}^2\left(\boldP, \boldQ ; \mathcal{H}_k\right) = \frac{1}{n(n-1)} \sum_{1 \leq i \neq i^{\prime} \leq n} k\left(\boldP_i, \boldP_{i^{\prime}}\right) &+\frac{1}{m(m-1)} \sum_{1 \leq j \neq j^{\prime} \leq m} k\left(\boldQ_j, \boldQ_{j^{\prime}}\right) \\
        &-\frac{2}{m n} \sum_{i=1}^n \sum_{j=1}^m k\left(\boldP_i, \boldQ_j\right)
    \end{aligned}
\end{equation}
Existing work for detecting high dimensional distribution shift using MMD estimators focus on the design of kernels.
\citeauthor{failloud} considers the squared exponential kernel $k(x, y)=\exp{\left(\sigma^{-1} \|x-y\|^2\right)}$, while~\citeauthor{liu2020learning} explore of use of deep neural networks for building more statistically powerful kernels.

While MMD presents an estimator for measuring the difference in distributions, it does not directly lead to a rigorous two sample test with a bounded significance level, to achieve this we will need to turn to the methodology of permutation testing presented in the next subsection.

\subsection*{Permutation Testing}
Permutation testing~\citep{fisher_1991} is a method to guarantee the significance level of any test by first estimating a quantile of the distribution of a test statistic when the null hypothesis $\mathcal{H}_0$ is true ($\Pc=\Qc$).
For some tests like KS and Binomial this quantile can be computed analytically or using an exact algorithm, but in the general case it cannot, motivating the need for alternative methods.

Suppose we have a test statistic function $T: \mathcal{X}^n\times \mathcal{X}^m\to \mathbb{R}$.
In the context of two sample testing $T$ maps a set $\boldP$ and $\boldQ$ to a real value which we assume to be some heuristic measure
of the distributional difference between $\Pc$ and $\Qc$ based on the finite sample observations.
To construct a one-sided two sample test with $T$ we must first determine a significance threshold $\tau$ such that when $\Pc=\Qc$ we have a probability of at least $1-\alpha$
that $T(\boldP, \boldQ) < \tau$ (or $T(\boldP, \boldQ) > \tau$ depending on the direction of the test).
To estimate $\tau$ we take advantage of the exchangeability of samples under $\mathcal{H}_0$.
If indeed $\Pc=\Qc$ then it should not matter which group of the total $n+m$ samples we call $\boldP$ and which we call $\boldQ$, so the strategy is to
combine, permute and redivide $\boldP$ and $\boldQ$ and feed them into $T$ many times, computing $\tau$ as the $1-\alpha$ quantile of all observations of the test statistic.
If the test performed on the original partitions exceeds $\tau$ (e.g., $T(\boldP, \boldQ) \geq \tau$) we can rule out $\mathcal{H}_0$ with an exact significance level $\alpha$.
We note that as we are estimating $\tau$ from finite data, it itself incurs statistical estimation error which in turn adjusts the true significance level of a test~\citep{NEURIPS2019_8fb21ee7}.
This is a minor detail which is ignored by several modern approaches for two sample testing that employ permutation tests~\citep{liu2020learning, zhao2022comparing}.


\section{Approaches for High Dimensional Data}\label{sec:cov-high-dimensions}
\subsection*{Dimensionality Reduction Approaches}
Many methods for detecting shift in high-dimensional data simply apply dimensionality reduction (DR) techniques followed by standard two sample tests~\citep{failloud}.
\citeauthor{failloud} perform an evaluation on several DR techniques including, PCA, random feature projections, trained and untrained auto-encoders and the output features of a classification model.
On each DR method they test the performance of univariate KS-tests on each dimension of the reduced data distribution aggregated using Bonfferoni correction as well as MMD using the squared exponential kernel.
\citeauthor{failloud}'s main takeaway is that using the softmax outputs of a pretrained classifier as low dimensional representations for performing KS-tests, a method known as black box
shift detection (BBSD)~\citep{bbsd}, is effective at confidently identifying several synthetic covariate shifts in imaging data (e.g., crops, rotations) given approximately 200 i.i.d samples.
However, applying statistical tests to non-invertible representations of data can never guarantee to capture arbitrary covariate shifts, as there may always exist multiple distributions that collapse to
the same test statistic~\citep{failuresofgen}.
For instance if considering PCA as the DR of choice an adversary could construct a new data distribution $\Qc$ where samples are drawn by transforming samples from $\Pc$ randomly within the null space of the orthogonal projection.

\subsection*{Learning Theoretic Approaches}
\cite{domainrep} introduces the earliest theoretic framework for identifying and bounding the effect of covariate shift based on discriminative learning with finite samples.
In the foundation work ``Detecting Change in Data Streams'', \citeauthor{atheory} introduce the concept of $\mathcal{A}$-distance as a generalization of the total variation to an arbitrary collection of measurable events $\mathcal{A}$.
\begin{equation}
    d_{\mathcal{A}}\left(\mathcal{P}, \mathcal{Q} \right)\triangleq2 \sup _{A \in \mathcal{A}}\left|\operatorname{Pr}_{\mathcal{D}}[A]-\operatorname{Pr}_{\mathcal{D}^{\prime}}[A]\right|\label{eq:total-var}
\end{equation}
The authors show how various choices of $\mathcal{A}$ can induce certain desired properties, for example when $\Pc$ and $\Qc$ are distribution over the real line and $\mathcal{A}$ is the collection of all intervals $(-\infty, \cdot)$
the $\mathcal{A}$-distance becomes equivalent to the Kolmogorov-Smirnov statistic.
\citeauthor{atheory} go on to prove tight bounds on various instantiations of the $\mathcal{A}$-distance for detecting shifts in an online setting.

\citeauthor{domainrep} develop a learning theoretic extension of \citeauthor{atheory} to show that when they chose a class of events whose characteristic
functions are functions from a set of binary classifiers $F$, the $\mathcal{A}$ distance in connection with VC theory~\citep{vapnik95}
allows for finite sample generalization bounds on the performance of arbitrary decision models from $F$ under covariate shift.
\citeauthor{domainrep} go on to show that the $\mathcal{A}$ distance defined for a binary function class $F$ is equal to
\begin{equation}
    d_F(\Pc, \Qc) \triangleq 2\left(1-2 \min_{f\in F}\text{err}(f)\right)\label{eq:a-dist}
\end{equation}
where $\min_{f\in F}\text{err}(f)$ is the minimum error that a domain classifier from $F$ can achieve on the task of distinguishing samples from $\Pc$ and $\Qc$
(i.e., if $\Pc=\Qc$ the best domain classifier will have error of 0.5 and $d_F(\Pc, \Qc)=0$ and if $\Pc$ and $\Qc$ can be perfectly discriminated by some $f\in F$ the $d_F(\Pc, \Qc)$ is maximized and equal to $2$).
This conclusion motivates the classifier two sample test (CTST)~\citep{paz2017revisiting} to detect shift by first training a domain classifier $d: \mathcal{X}\to \{0,1\}$ on a set of samples from $\Pc$ and $\Qc$ then
running a binomial test to reject the null hypothesis that $d$ achieves an accuracy of 0.5 on unseen data.
\citeauthor{domainrep} inspired many ideas for unsupervised domain adaption including the popular method \textit{domain adversarial domain adaptation (DANN)}~\citep{ganin2016domain}
which aims to learn robust representations of data for classification by explicitly regularizing for a small $d_F$ on the representation space.

\subsubsection*{Learning Model Based Approaches}
More recent approaches for covariate shift detection including, deep kernel MMD~\citep{liu2020learning} and H-Divergence~\citep{zhao2022comparing} focus on training learning models with objectives optimized specifically for statistical testing in a way that goes beyond CTSTs.

\citeauthor{liu2020learning} show how to train a deep feature extractor $\phi: \mathcal{X}\to\mathbb{R}^d$ to
optimize the power of an MMD test using a simple kernel (e.g., squared exponential) whose inputs are feature vectors tom $\phi$.
\citeauthor{zhao2022comparing} introduce a new family of distributional divergences (H-Divergences) that builds off the concept of H-entropy~\citep{10.1214/aoms/1177704567}
\[H_\ell \triangleq \inf_{a\in \mathcal{A}}\mathbb{E}_{X\sim\Pc}[\ell(X, a)]\]
The H-entropy is defined on a distribution $\Pc$ as the optimal expected action with respect to loss function $\ell$ and set of possible actions $\mathcal{A}$.
For example if $\ell$ if the reconstruction loss of an autoencoder $f_\theta$ and $\mathcal{A}$ is the set of all possible parameter configurations $\theta$, $H_\ell$ corresponds to the expected reconstruction
loss of $f_{\theta^\star}$ where $\theta^\star$ is the optimal choice of parameters.

\citeauthor{zhao2022comparing} present a general definition of the H-Divergence using a function whose arguments are restricted to be $H_\ell\left( \frac{p+q}{2} \right) - H_\ell(p)$ and $H_\ell\left( \frac{p+q}{2} \right) - H_\ell(q)$.


In our work we take a transductive learning approach and construct a method to directly use the structure of a supervised classification problem to improve the statistical power for detecting shifts.


\section{Out of Distribution Detection}\label{sec:out-of-distribution-detection}
Out of distribution (OOD) detection focuses on identifying when a specific data point $x'$ admits low likelihood under the original training distributions
$(p_{\text{train}}(x') \approx 0)$---a useful tool to have at inference time.~\cite{densityratio, densityofstates} represent a broad class of work that uses density estimation
to pose the identification of covariate shift as anomaly detection.
However, in finite samples, density estimation for high-dimensional data can be difficult which in turn affects the accuracy of anomaly detection~\citep{failuresofgen}.
Still the discipline of OOD detection has seen several recent successes, including ODIN~\citep{ODIN}, Deep Mahalanobis Detectors~\citep{mahalano}
and, Gram Matrices~\citep{grammat} which all directly use the predictive model (e.g., information from the intermediate representations of neural networks) to create a real valued score function $\phi: X\to \mathbb{R}$
which attempts to map data points to a real number near zero if the datapoint is in the training distribution and far from zero if the datapoint is out of distribution.
Such methods are largely based on heuristics on the manifold of neural networks offering little to no theoretical guarantees on detecting subtle types of covariate shifts encountered in real-world settings.
Furthermore, the majority of methods in this space have been designed exclusively for deep neural networks, an uncommon modelling choice particularly for tabular data~\citep{tabdatasurvey}.

Despite these shortcomings, OOD methods can be readily applied for the problem of covariate shift detection under the same principle that governs Integral Probability Metrics (IPMs)~\citep{IPMS}; namely that if two
distributions are identical, any function should have the same expectation under both distributions.
Hence testing for a difference in expected OOD scores between a source and target dataset is a valid hypothesis test for covariate.
However, as OOD scores are designed to only look at a single sample level, instead of treating a set of samples as a whole one would not expect such a test to yield high statistical power for near distribution shifts.


\section{Uncertainty Estimation}\label{sec:uncertainty-estimation}
Related to OOD, uncertainty estimation concerns developing models that identify sources of uncertainty in their predictions~\citep{ensemble, trustuncert}.
A non exhaustive list of approaches to model uncertainty using predictive models are Gaussian Processes~\citep{williams1995gaussian}, measuring the
predictive variance or entropy of Deep Ensembles~\citep{ensemble} or Monte Carlo Dropout~\citep{dropout}, Bayesian Neural Networks~\citep{baysneural}, and Evidential Learning~\citep{evidential, evclass}.
Naturally, uncertainty should be large when samples are OOD implying that one could consider testing for covariate shift using the same principle as identified for OOD scores.
However, ~\citep{trustuncert} perform a large-scale empirical comparison of uncertainty estimation methods and find that while deep ensembles generally provide the best results, the quality of uncertainty estimations, regardless of method, consistently degrades with increasing covariate shift.


\section{Selective Classification and PQ Learning}\label{sec:selective-classification-and-pq-learning}
Selective classification concerns building classifiers that may either predict on or reject on test samples~\citep{selectivenet}.
% One can turn a method for OOD detection or uncertainty estimation into a selective classifier.
% However,~\citep{failuresofgen} show that this is often not guaranteed to be valid for arbitrary distribution shifts.
Recent work by~\citep{pqlearn} develops a formal framework known as PQ learning which extends probably approximately correct (PAC) learning~\citep{Haussler90probablyapproximately} to arbitrary test distributions by allowing for selective classification.
While PAC learning concerns the development of a classifier with a bounded finite-sample error rate on its training distribution, PQ learning seeks a selective classifier with jointly bounded finite-sample error and rejection rates on arbitrary test distributions.
The Rejectron algorithms proposed therein builds an ensemble of models that produce different outputs relative to a perfect baseline on a set of unlabeled test samples.
% While the original algorithm was designed for binary classification in the noiseless setting, follow-up work~\citep{slicendice} has relaxed the latter assumption to allow for one-sided noise.
We provide a summary of the original Rejectron algorithm in the supplementary material (see \autoref{sec:rejectron}).
PQ-learning represents a major theoretical leap for learning guarantees under covariate shift;
however, the majority of the underlying ideas have not been implemented/tested experimentally using real-world data.
We show how to build a PQ learner by generalizing the Rejectron algorithm, overcoming several limitations and assumptions made by the original work including extending beyond simple binary classification to general multiclass/multilabel tasks and reducing the number of samples required for learning at each iteration.
We go on to show how a PQ learner can be used to characterize covariate shifts in real-world data.
