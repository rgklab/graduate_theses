\section{Problem Setup}\label{sec:problem-setup}
Let $f: X\to \mathbb{R}^N $ be a classifier from a function class $F$ that maps from space of covariates $X$ to a discrete probability distribution over classes $Y=\{1,\ldots,N\}$ i.e.,
\[f(x)=[f(x)_1, f(x)_2, \ldots, f(x)_N]^\top\text{ s.t. }f(x)_i \geq 0\ \forall\ i\in Y \text{ and } \|f(x)\|_1 = 1\]
We denote the prediction of a model $f(x)$ as $\hat{f}(x)\coloneq \argmax_{i\in \{1,\dots,N\} }f(x)$ which corresponds to the class with the maximum predicted probability.
We assume $f$ was trained on a dataset of \textbf{labeled} samples $\boldP = \{(x_i,y_i)\}_{i=1}^n$ where each $x_i$ is drawn identically from a distribution $\Pc$ over $X$, and
each label $y_i \in Y$ corresponds to the ground truth label for the classification problem of interest.
In deployment, $f$ is made to predict on new \textbf{unlabeled} samples $\boldQ = \{\tilde{x}_i\}_{i=1}^m$ ($\tilde{x}_i$ from a distribution $\Qc$ over $X$.
Our high level goal is to determine whether $f$ may be trusted to do so accurately.
More specifically, the problem we address is how to automatically detect, with high statistical power, from only a \textit{small} set of samples $\boldQ$,
if the new covariate distribution $\Qc$ has shifted from $\Pc$.
In addition, we wish to detect true shifts efficiently and with a provably correct lower bound on false positive rate (the rate at which we detect shift when in reality there is no shift).
%The false positive rate is also referred to as the probability of making a type II error, or the significance level and is denoted by the symbol $\alpha$.

Our problem will differ from a standard two sample testng scenario in several critical ways (1) we may assume access to many more samples from $\Pc$
compared to $\Qc$; motivated by the setting where we wish to detect shift quickly, and hence from as few samples as possible
(2) we have access to a robust classifier $f$ that performs \textit{reasonably well} (e.g. could realistically be deployment) on held out samples from $\Pc$ as well as the learning algorithm $L$ used to train it.
(3) We seek to only detect shifts that could reduce classification robustness of $f$ --- we refer to this shift characterization \textit{harmful covariate shift} and elaborate on it in the next section.

\clearpage
\section{Harmful Covariate Shift}\label{sec:harmful-covariate-shift} A shift in the data distribution is not always harmful.
In many practical problems, a practitioner may use domain knowledge to embed invariances with the explicit goal of ensuring the predictive performance of a classifier does not, by construction, change under certain shifts.
This may be done directly via translation invariance in convolutional neural networks, permutation invariance in DeepSets~\citep{deepsets} or indirectly via data augmentation or domain adaptation.
Other model invariances are often learned naturally due to the structure of a given dataset and associated decision problem.

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-8mm}
    \begin{center}
        \includegraphics[width=\linewidth]{images/mnist_corrupted.pdf}
    \end{center}
    \vspace{4mm}
    \caption{\small Blacking out the corners in MNIST is an example of a \textit{non harmful} covariate shift with respect to a simple CNN that achieves near 100\% accuracy on both sets.
    A likely explanation for why this shift is not harmful is because MNIST images contain content primarily in the center, leading to classifiers implicitly learning an invariance to the corners
    of the image.}
    \label{fig:mnist}
    \vspace{-8mm}
\end{wrapfigure}
For example, we train a simple CNN using the LeNet architecture on the MNIST dataset as $\boldP$ and test it on a modified version where a $5\times 5$ pixel region at each corner of image is blacked out $\boldQ$ (see \autoref{fig:mnist}).
We find that when tested on a set of $10,000$ unseen examples the accuracy ($\pm$ standard error) on the original images is $98.48\pm 0.12 \%$ and $98.09\pm 0.14 \%$ on those with blacked out corners (i.e., nearly identical).
A plausible explanation for this observation is that the content useful for explaining MNIST is found near the center of the image,
which encourages a robust classifier to learn invariance to the content of the image near the corners.
However, in terms of pure statistics one could not argue that there is no covariate shift between $\Pc$ and $\Qc$.
To support this claim we run the simplest possible two sample test where we take the mean intensity of each image in $\boldP$ and $\boldQ$ using only a sample size of $|\boldP|=|\boldQ|=10$ and
run a Kolmogorov Smirnov Test~\citep{kstest} over $1,000$ permutations to calculate a test power of $0.981\pm 0.004$ at the strongest significance level $\alpha=0$.

The MNIST example motivates the point that practical heuristics, and problem structure can lead to models generalizing to a more broad range of distributions than can be characterized by just the training set.
And furthermore, that when two sample tests are decoupled with respect to the heuristics and structures of the domain, they lose their informative abilities.
To formalize this notion we define the informal idea of the \textit{generalization set} $\Rc$ as the set of distributions, beyond simply the training distribution, that a model generalizes to.
$\Rc$ is an abstract object which may in general depend on the model architecture/function class $F$, learning algorithm $L$ and training dataset $\boldP$.
While $\Rc$ is difficult if not impossible to characterize exactly, we seek a practical method for detecting shift that is explicitly tied to $\Rc$.

Our approach is based both on PQ learning, from learning theory and learning dynamics.
A one sentence summary is: if we can find a set of classifiers $\{g_1,\ldots, g_\aleph\}\subseteq F$ that achieve the same generalization
set as $f$ but behave inconsistently with respect to $f$ on samples from a distribution $\Qc$, then $\Qc$ must not be a member $\Rc$.
Since it is not possible to know $\Rc$ for a given classifier we introduce a more practical definition of harmful covariate shift
based on the learning algorithm and source dataset --- whose complex interaction is what induces $\Rc$.

\begin{definition}
    [ Harmful Covariate Shift (HCS) ]
    Let $F$ be a family of decision functions that are learnable via a learning algorithm ${L}$ from a set of samples drawn from a source distribution $\Pc$.
    We say a covariate shift from distributions $\Pc \to \Qc$ over $X$ is $(\ell, \alpha, L, \Pc)$-harmful,
    if there exists a finite subset of models of two or more models $\mathbf{f} \subseteq F$ that achieve a source domain loss $\mathbb{E}_{\Pc}[\ell(f(x), y)] \leq \alpha$ for all $f\in \mathbf{f}$ while
    being more likely to disagree with each other on an unseen sample from $\Qc$ compared to $\Pc$.
    \begin{equation}
        \begin{aligned}
            \exists\ \mathbf{f}\subseteq F, &\text{ s.t. } \forall f\in \mathbf{f}\ \ \mathbb{E}_{\Pc}[\ell(f(x), y)] \leq \alpha \text{ and } \\
            &\mathbb{P}_{x\sim \Qc}(\exists\ f_i, f_j \in \mathbf{f}\text{ s.t. }\hat{f}_i(x)\neq \hat{f}_j(x)) > \mathbb{P}_{x\sim \Pc}(\exists\ f_i, f_j \in \mathbf{f}\text{ s.t. }\hat{f}_i(x)\neq \hat{f}_j(x))
        \end{aligned}\label{eq:hcs}
    \end{equation}
    In plainer words, we define harmful covariate shift based on the existence of multiple \textit{good} models on $\Pc$ that we assume learn the same generalization set, but
    that tend to disagree on $\Qc$ with greater probability compared to $\Pc$.
\end{definition}


\section{Constrained Disagreement Classifiers}\label{sec:constrained-disagreement-classifier-(cdc)}
Our strategy for detecting HCS will be to create an ensemble of \textit{constrained disagreement classifiers} $\{g_1, \ldots, g_\aleph \}$,
classifiers created by the same learning algorithm as $f$ that are constrained to predict consistently (i.e., predict the same as $f$) on $\boldP$ but as differently as possible on $\boldQ$.
If $\Qc$ is within $\Rc$ then such an ensemble will fail to predict differently, as the invariances induced by $L$ and $\boldP$ by definition do not allow a high degree of uncertainty within $\Rc$.
However, when we can find an ensemble that exhibits inconsistent behaviour on $\Qc$, there must be covariate shift that explicitly lies outside $\Rc$.
To make the idea of constrained disagreement classifiers tangible we propose a simple definition which we will translate into a learning algorithm in the following sections.

\begin{definition}
    \label{def:cdc}
    [Constrained Disagreement Classifier (CDC)]
    A constrained disagreement classifier $g_{(f, \boldP, \boldQ)}$, or simply $g$ or $g_\boldQ$ if $f$, $\boldP$ and $\boldQ$ are clear with context, is a classifier with the following properties:
    \begin{enumerate}
        \item $g$ belongs to the same model class as $f$ and is updated with the same learning algorithm when it observes samples from $\Pc$;
        \item $g$ achieves similar training and held out performance on $\Pc$ with respect to $f$;
        \item $g$ disagrees maximally with $f$ on elements of dataset $\boldQ$ while not violating 1 and 2.
    \end{enumerate}
    Our definition of a CDC aims to explicitly capture the concept of a classifier that learns the same generalization region as $f$ while behaving as inconsistently as possible on $\boldQ$.
\end{definition}


\section{Domain Classification and Model Capacity}\label{sec:relationship-with-learning theory}
In our background on covariate shift (\autoref{sec:cov-high-dimensions}) we discussed the mathematical concept of $\mathcal{A}$ distance from~\citep{atheory, domainrep}
and how it translates to the classifier two sample test~\citep{paz2017revisiting}.
Naively, the idea of training classifiers to disagree on out of domain data is identical to the simpler concept of domain classification when given models with sufficiently a large learning capacity e.g., deep neural networks.
However, vanilla domain classification does not leverage the structure and invariance of the specific problem, which will in general result in a less informative test as we have motivated previously (\autoref{sec:harmful-covariate-shift}).
In \autoref{fig:hcs} we explore in a visual toy example how the capacity of CDCs influences the sensitivity for detecting certain distribution shift.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{images/hcs.pdf}
    \caption{\small Investigating the relation between CDC model complexity and the ability to identify covariate shift.
    We consider a toy example where the ground truth labels are generated using a quadratic decision boundary, shown as a black dashed line.
    The blue points correspond to training samples, and the orange and green points correspond to two different covariate shifts, one closer to the training distribution and the other further.
        (Left) When we choose an underspecified model family (e.g., linear classifiers), there exists no CDC that reports different explanations for the orange and green points.
        However, if expert knowledge led us to believe that a linear classifier is the true causal predictor for the entire domain, we would consequently not be worried about covariate shift.
        (Center) When we choose a quadratic function family, there exists enough variation within the space of models that explain the training set to offer different explanations on the distance shift (orange) but not on the near shift (green).
        An analogy to a more complex learning problem would be that the green region represents a set of datapoints shifted from the source, yet still within the generalization/invariance set of the learning algorithms.
        (Right) When we learn from an overly expressive function family (polynomials of degree 3+), the space of models that explain the training set can offer different explanations of even near covariate shifts (green points).}
    \label{fig:hcs}
\end{figure}

Beyond the concept of shift harmfulness, we will see in our methods and empirical study that unlike domain classifiers, CDCs can easily leverage a pretrained model to result in significantly larger statistical power when detecting shift.


\section{Connection to PQ Learning}\label{sec:relationship-with-pq-learning}

As our work builds on PQ learning, we provide a summary of the original framework and clearly state the distinctions in our methodology.
In PQ learning we seek a selective classifier $\left. h\right|_{S}$ that achieves
a bounded tradeoff between its in distribution rejection rate $\text{rej}_h(\mathbf{x})$ and its out of distribution error $\operatorname{err}_h(\tilde{\mathbf{x}})$ with respect to a ground truth decision function $d$.
Formally, a selective classifier predicts according to a base classifier $h$ if the input $x$ is inside some set $S$ and otherwise rejects/abstains from predicting.
\begin{equation}
    \left. h\right|_{S}(x) \coloneqq \begin{cases}
                                         h(x) & x\in S \\
                                         \text{reject} & x\not\in S
    \end{cases}
    \label{eq:selective_classifier_defn}
\end{equation}
The error and rejection rate of a selective classifier are defined for a selective classifier $\left. h\right|_{S}$ with respect to a distribution $\mathcal{P}$ as:
\begin{equation}
    \text{rej}_{h}(S) \coloneqq \mathbb{P}_{x\sim\mathcal{P}}\left[x\not \in S\right]\quad \text{err}_h(S) \coloneqq \mathbb{P}_{x\sim\mathcal{P}}\left[h(x) \neq y\land x\in S\right]
\end{equation}
Where $y=d(x)$ is the ground truth label for $x$.
The \textit{empirical} rejection and error rates for a set of samples $\mathbf{x}=\{x_i\}_{i=1}^n$ and corresponding labels $\mathbf{y}=\{y_i\}_{i=1}^n$ are similarly defined as:
\begin{equation}
    \text{rej}_{h}(\mathbf{x}) \coloneqq \frac{1}{n}\sum_{i=1}^n \delta_{x_i\not \in S}\quad \text{err}_h(\mathbf{x}) \coloneqq \frac{1}{\sum_{i=1}^n x_i\in S} \sum_{i=1}^n \left(\delta_{x_i\neq y_i} \times \delta_{x_i\in S}\right)
\end{equation}
\noindent
Using this construction \citeauthor{pqlearn} define PQ learning with the following learning theoretic bound.

\begin{definition}[{PQ learning~\citep{pqlearn}}]
    Learner $L$ $(\epsilon, \delta, n)$-PQ-learns $F$ if for any distributions $\Pc, \Qc$ over $X$ and any ground truth function $d \in F$, its output $h\coloneqq L(\boldP, d(\boldP), \boldQ)$ satisfies
    \begin{equation}
        \mathbb{P}_{{\mathbf{x} \sim \Pc^{n}},\ {\tilde{\mathbf{x}} \sim \Qc^{n}}}\left[\text{rej}_h(\mathbf{x})+\operatorname{err}_h(\tilde{\mathbf{x}}) \leq \epsilon\right] \geq 1-\delta
        \label{eq:pqlearn}
    \end{equation}
    $L$ PQ-learns $F$ if $L$ runs in polynomial time and if there is a polynomial $p$ such that $L$ $(\epsilon, \delta, n)-$PQ learns $F$ for every $\epsilon, \delta>0, n \geq p(1 / \epsilon, 1 / \delta)$.
\end{definition}
\citeauthor{pqlearn} propose the Rejectron algorithm for PQ learning in a noiseless binary classification setting with zero training error and access to a perfect empirical risk minimization (ERM) oracle.
Rejectron sequentially searches for new classifiers that achieve zero training error while attempting to predict the opposite label on subsets of the unlabeled set $\boldQ$.
We highlight several pitfalls that prevent a practical realization of Rejectron (1) the classification problem must be binary (2) the optimization objective at each step requires an ERM query with $\Omega(|\boldP|^2)$ samples and (3) most significantly there is no process to control overfitting of large capacity models.
By tackling the above issues we derive a practical PQ learning algorithm and propose a powerful hypothesis test to leverage the PQ learner to identify harmful covariate shift.


\section{Learning to Disagree}\label{sec:dis}
To train a classifier to disagree in the binary setting, it suffices to flip the labels.
However, in the multi-class classification, it is unclear what a good objective function is.
We formulate an explicit loss function that can be minimized via gradient descent to learn a CDC.
For classification problems, letting $g(x_i)$ be the predictive distribution over $N$ classes, $\hat{f}(x_i)\in \{1,\dots N\}$
the label predicted by $f$ and $\mathbb{I}[\cdot ]$ a binary indicator, we define the \textit{disagreement-cross-entropy} (DCE) $\small \tilde{\ell}$ as:
\begin{equation}
    \tilde{\ell}_{\text{DCE}}(g(x_i), f(x_i)) =  \frac{1}{1-N} \sum_{c=1}^N \mathbb{I}[{\hat{f}(x_i) \neq c}] \log(g(x_i)_c)
    \label{eq:anitcross}
\end{equation}
$\tilde{\ell}$ corresponds to taking the cross entropy of $\hat{y}$ with the uniform distribution over all classes except $f(x_i)$.
Since the primary criteria is that $g(x_i)$ disagrees with $f(x_i)$, $\tilde{\ell}$ is designed to minimize the probability of $g$'s prediction for the output of $f$'s while maximizing its overall entropy.
Our definition of $\tilde{\ell}$ is significantly more stable to optimize compared to simply maximizing the regular cross entropy as it has a bounded global minimum.

Our goal is to agree on $\boldP$ and disagree on $\boldQ$.
Consequently, we learn with the loss in \autoref{eq:loss}. $\ell$ denotes the standard cross entropy loss and $\tilde{\ell}$ is the disagreement cross entropy. $\lambda$ is a scalar parameter that controls the trade off between agreement and disagreement.
\begin{equation}
    \mathcal{L}_{\text{CDC}}(\boldP,\boldQ) = \frac{1}{|\boldP|+|\boldQ|}\left(\sum_{(x,y)\in \boldP} \ell(g(x), y) + \lambda \sum_{\tilde{x}\in \boldQ} \tilde{\ell}_{\text{DCE}}(g(\tilde{x}), f(\tilde{x})) \right)
    \label{eq:loss}
\end{equation}
When learning CDCs in practice, $\mathcal{L}_\text{CDC}$ should be combined with any additional regularization and data augmentation used in the original training process of $f$ to ensure that we retain the true generalization region of $f$.
Furthermore, training and validation metrics must be closely monitored on unseen samples from $\Pc$ to ensure that $g$ achieves similar generalization performance on $\Pc$.

\subsection{Choosing $\lambda$}
%In the original formulation of Rejectron, selective classifiers are trained on a dataset consisting of $\boldP$ replicated $|\boldP|$ times and $\boldQ$.
%Calling an ERM oracle on this data ensures that a misclassification on $\boldP$ is significantly more costly than one on $\boldQ$ but requires $\Omega(|\boldP|^2 )$ samples, an impractical number for large datasets.
We show that we can choose the scalar parameter $\lambda$ in \autoref{eq:loss} to set learning $\boldP$ as the primary learning objective and only when it cannot be improved, we allow $g$ to learn how to disagree on $\boldQ$,
thus relaxing the original constrained optimization objective to a tractable weighted objective.
The reasoning is a simple counting argument.
Suppose agreeing on each sample in $\boldP$ incurs a reward of $1$ and disagreeing with each sample in $\boldQ$ a reward of $\lambda$.
To encourage agreement on $\boldP$ as the primary objective, we set $\lambda$ such that the extra reward obtained by going from \textit{zero} to \textit{all} disagreements on $\boldQ$ is less than that achieved with only one extra agreement on $\boldP$, this gives $\lambda|\boldQ| < 1$.
Practically, we chose $\lambda=1/(|\boldQ| + 1)$ and find that no tuning is required.

\subsection{Generalizing Beyond Cross Entropy}\label{subsec:gen_beyond}     When training models with arbitrary discrete or generally non-differentiable with respect their objective (e.g., random forest), we must find a more general solution for creating CDCs.
Such a solution should (1) reduce to the DCE when the model is, in fact, continuous and trained using the standard cross-entropy, and (2) reduces to label flipping when $N=2$ (binary classification).
Our simple solution is to replicate every sample in $\boldQ$ exactly $N-1$ times and create a unique label for each
from the set $\mathcal{S}\coloneqq \{1,\dots,N\}\backslash \{t\}$ where $t$ is the disagreement target.
We also give each a sample a weight of $1/(N-1)$.
In the case of $N=2$, this corresponds to no replication and simply assigning the opposite label.
In the case where the model learns by cross-entropy, this generalization corresponds to the DCE in \autoref{eq:anitcross}; we provide a simple proof below.
\smallbreak\noindent
\begin{proof}
    Starting with the definition of the cross entropy
    \begin{equation}
        \text{CE}(f(x), y) = -\sum_{c=1}^N \mathbb{I}[{c=y}] \log(f(x)_c)
    \end{equation}
    Now we consider the sum of the cross entropy for each label in $\mathcal{S}$:
    \begin{align}
        \sum_{y\in \mathcal{S}} \text{CE}(f(x), y) &= -\sum_{y\in \mathcal{S}} \sum_{c=1}^{N}\mathbb{I}[{c=y}]\log(f(x)_c) \\
        &=-\sum_{c=1}^{N}\mathbb{I}[{c\in \mathcal{S}}]\log(f(x)_c)\\
        &= (N-1) \tilde{\ell}_\text{DCE}(f(x), y)
    \end{align}
    Hence when giving each sample a weight of $1/(N-1)$ we recover the exact from of DCE.
\end{proof}

\subsection{Ensembling} To learn richer disagreement rules, we create an
ensemble of CDCs where the $k^{\text{th}}$ model is trained only to disagree on the subset of
$\boldQ$ that has yet to be disagreed on by models $1$ through $k-1$.
The final disagreement rate $\phi_\boldQ$ is the fraction of unlabelled samples where any CDC provides an alternate decision from $f$.
In what follows we use this rate to detect shift.

\begin{algorithm}
    \DontPrintSemicolon
    \SetAlgoLined
    \SetNoFillComment
    \SetKwRepeat{Do}{do}{while}%
    \caption{Constrained Disagreement}\label{alg:constrained}
    \KwIn{$L$: learning algorithm, $\boldP_{\text{train}}$: labeled training dataset $\{\dots,(x_i,y_i),\dots\}$,
        $\boldP_{\text{val}}$: labeled validation dataset $\{\dots,(x_i,y_i),\dots\}$, $\boldQ$: unlabeled test dataset $\{\dots,{x}_i,\dots\}$,
        $f$: classifier trained on $\boldP$, $\mathcal{M}$: evaluation metric (default \textit{accuracy}), $\varepsilon$: tolerance (default 0.05),
        $k$: max epochs (default 10)}
    \KwOut{Constrained Disagreement Classifier $g_{(f,\boldP, \boldQ)}$}
    $\hat{\boldQ}\ \gets\ \{({x}, f({x}))\ |\ \hat{x}\in \boldQ\}$ \tcp*{infer pseudo labels on $\boldQ$ using $f$}
    \tcp*{create a batched dataloader using $\boldP \text{ and } \boldQ$}
    ${\boldP\boldQ}\ \gets\ $Batched($\{(x,y)\ |\ (x,y)\in \boldP \land (x,y)\in \boldQ$\})\\
    $g\ \gets\ f$\tcp*{Initialize $g$ with $f$}
    $m_0\ \gets\ \mathcal{M}(f, \boldP_{\text{val}})$ \tcp*{Compute the validation performance of $f$}
    \While{$\mathcal{M}(f, \boldP_{\text{val}}) > m_0 - \varepsilon$ and iterations $<\ k$}{
        \tcp*{Training epoch over ${\boldP\boldQ}$}
        \For{batch $\ \in \boldP\boldQ$}{
            $x_P,\ y_P\ \gets\ \{(x, y)\ |\ (x,y)\in\text{ batch}\text{ and }(x,y)\in {\boldP}_{\text{train}}\}$\\
            $x_Q,\ y_Q\ \gets\ \{(x, y)\ |\ (x,y)\in\text{ batch}\text{ and }(x,y)\in \hat{\boldQ}\}$\\
            \tcp*{Update $g$ to minimize the loss in \autoref{eq:loss} and base loss $\ell$ defined by $L$}
            $g\ \gets \text{Update}(g, L, (x_P, y_P), (x_Q, y_Q))$
        }
    }
    \Return{$g$}
\end{algorithm}


\section{Detecting Shift with Constrained Disagreement}\label{sec:from-constrained-disagreement-to-detecting-shift-with-hypothesis-tests}
A natural way to apply the concept of constrained disagreement to the identification of covariate shift is
to partition $\boldQ$ into two sets, using the first to train a CDC ensemble and the second to compute an unbiased estimate
of its held out disagreement rate $\phi_\Qc$.
We would statistically compare this disagreement rate using a $2\times 2$ exact hypothesis test (e.g., Fisher's or Barnard's) against a baseline estimate for the disagreement rate on $\Pc$ computed using unseen data.
The following shows that this results in a provably correct method to detect shift with high probability using only finite samples.
\begin{theorem}[Disagreement implies covariate shift]
    \label{th:dis}
    Let $f$ be a classifier trained on dataset $\boldP$ drawn from $\Pc$ and their corresponding labels.
    Let $g$ be a classifier that is observed to agree (classify identically) with $f$ on $\boldP$ and disagree on a dataset $\boldQ$ drawn from $\Qc$.
    If the rate which $g$ disagrees with $f$ on $n$ unseen samples from $\Qc$ is greater than that from $n$ unseen samples from $\Pc$ with probability greater than $p^\star \coloneqq \frac{1}{2}\left(1- 4^{-n} \binom{2n}{n}\right)$ there must be covariate shift.
\end{theorem}
\textit{Sketch of Proof}.
We show that under the null hypothesis where $\Pc = \Qc$ the tightest upper bound on the probability that $g$ is more likely to disagree on $\Qc$ compared to $\Pc$ is $p^\star$.
The contrapositive argument then states
if we deem the probability to be greater than $p^\star$ there must be a covariate shift.
This result motivates a hypothesis testing approach to determine how probable it is that $g$ is truly more likely to disagree on $\Qc$ given only a set of finite observations.
The full proof can be found in \autoref{sec:proofs}.

\textit{Remark}.
While \autoref{th:dis} takes a frequentist's approach to identifying covariate shift, we show in \autoref{sec:proofs} (Theorem 2) that there is a Bayesian formulation which can provide a closed-form and informative lower bound on the probability
of covariate shift given an observation of finite sample disagreement rates on $\boldP$ and $\boldQ$.

Our theory, while simple, has a limitation that prevents its direct application.
Any approach that requires unseen samples from $\boldQ$ is ill-suited for the low data regime, as it requires splitting $\boldQ$ leaving an even smaller set
for computing the disagreement rate.
Estimators from small samples result in high variance and ultimately low statistical power.
Furthermore for small samples sizes the ability to find the classifier $g$ that meets of predicate of \autoref{th:dis} becomes incredibly difficult.
Since our objective is to detect covariate shift from as few test samples as possible, splitting $\boldQ$ and computing disagreement on unseen data will not result in a powerful test.
To tackle this issue practically, we take a transductive approach based on intuition from learning theory and observed dynamics of CDC learning.
We claim that training learning models to disagree on samples from $\boldQ$ while generalizing to $\Rc$ is a \textit{far easier} task when $\Qc$ is not in $\Rc$.
Easiness in this setting is captured by the fact that standard learning algorithms tend to converge significantly quicker to the CDC objective when $\Pc \neq \Qc$.
We can therefore use the \emph{relative increase} in disagreement between CDCs \textit{during training} on $\boldQ$ and $\boldP$ to capture a quantity that is more informative than the unbiased statistic without reducing samples from $\boldQ$ that we can use.


\section{The Detectron Test}\label{sec:the-detectron-test}

\subsection{Detectron Disagreement}
Our proposed method is to train two CDC ensembles of size $\aleph$
denoted by $\mathbf{g}_{\boldQ}\coloneqq \{g_{\boldQ}^1,\ldots {g}_{\boldQ}^\aleph\}$ and $\mathbf{g}_{\Pun}\coloneqq \{g_{\Pun}^1,\ldots g_{\Pun}^\aleph\}$.
First, $g_{\boldQ}$ is trained to disagree with $f$ on $\boldQ$, while constrained to agree on the original training set $\boldP$.
Next, enforcing the null hypothesis, we train $\mathbf{g}_{\Pun}$ to disagree on set of \textbf{unseen samples} $\Pun$ drawn from $\Pc$ where $m\coloneqq|\Pun|=|\boldQ|$.
To maximize sample efficiency, we take a transductive approach to detecting shift by analyzing the outputs of $\mathbf{g}_{\boldQ}$ and $\mathbf{g}_{\boldP}$ on the sets $\boldQ$ and $\Pun$ themselves by
calculating their \textit{empirical disagreement} during training.

We define the empirical disagreement between a base classifier $f$, an ensemble of classifiers $\mathbf{g}\coloneqq \{g^1,\dots,g^\aleph \}$, and an unlabeled dataset $\mathbf{D}\coloneqq \{x_i\}_{i=1}^n$ as the proportion of samples in $\mathbf{D}$
where any $g\in \mathbf{g}$ predicts differently from $f$:
\begin{equation}
    \phi_{\mathbf{D}}(f,\mathbf{g}) \coloneqq \frac{1}{|\mathbf{D}|} \sum_{x\in \mathbf{D}} \left(1-\mathbb{I}\left[\bigwedge\limits_{g\in \mathbf{g}} \hat{f}(x)=\hat{g}(x)\right]\right)
\end{equation}
The fraction of samples that $\mathbf{g}_{\Pun}$ disagrees on with respect to $f$ on $\Pun$ is denoted using shorthands $\phi_{\Pun}\coloneqq \phi_{\Pun}(f, \mathbf{g}_{\Pun})$,
and similarly for $g_{\boldQ}$ as $\phi_{\boldQ}$.
Note that $\phi_{\Pun}$ and $\phi_{\boldQ}$ are random variables that obey intractable distributions governed by the dynamics of the CDC learning algorithm and disagreement sets $\Pun$ and $\boldQ$.
The observed low variance and large divergence between the distributions of $\phi_{\Pun}$ and $\phi_{\boldQ}$ under covariate shift forms the backbone the \method test.
Under the null hypothesis $\mathcal{H}_0$, if $\Qc$ is not a harmful shift from $\Pc$,
then the intuition we have built suggests that $\phi_{\boldQ}$ will not grow more rapidly than $\phi_{\Pun}$
\begin{equation}
    \mathcal{H}_0: \mathbb{E}[\phi_\boldQ] \leq \mathbb{E}[\phi_\Pun], \quad  \mathcal{H}_a: \mathbb{E}[\phi_\boldQ] > \mathbb{E}[\phi_\Pun]
    \label{eq:harmfulshifttest}
\end{equation}
The converse of harmful shift is expressed as the one-sided alternative $\mathcal{H}_a$, meaning disagreeing on $\boldQ$ \textit{is easier} than $\Pun$.
See \autoref{fig:harmfulshift} for a visual depiction of CDC training dynamics when applied to the near distribution shift benchmark CIFAR 10/10.1~\citep{cifar101}.

\begin{figure}[!htb]
    \floatbox[{\capbeside\thisfloatsetup{capbesideposition={right,top},capbesidewidth=0.4\linewidth}}]{figure}[\FBwidth]
    {\caption{\small \textbf{CDC Training Dynamics:} In blue we train CDCs to disagree on a set of 100 samples from CIFAR 10.1~\citep{cifar101} ($\boldQ$) -- a near OOD test set for CIFAR 10 -- while in black
    we force CDCs to disagree on the original CIFAR 10 test set ($\Pun$).
    Even after a small number of training batches the empirical disagreement rate grows significantly larger CIFAR 10.1 compared to CIFAR 10.
    The observation that CIFAR 10.1 results in more predictive variance under the CDC algorithm suggest that this covariate shift is harmful}
    \label{fig:harmfulshift}}
    {\includegraphics[width=\linewidth]{images/training_cdcs_small.pdf}}
\end{figure}
We refer to the test in \autoref{eq:harmfulshifttest} as \textit{\method\ Disagreement} (Det-D).
To compute the test result at a significance level $\alpha$ we first estimate the null distribution of $\phi_\boldP$ for a fixed sample size $n$ by training $K$ calibration rounds of $g_{\boldP}$ with different random sets $\Pun$ of size $n$.
The test result is significant if the observed disagreement rate $\phi_{\boldQ}$ is greater than the $(1-\alpha)$ quantile of the null distribution.
This process is explained visually in \autoref{fig:detectron_permutation}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{images/detectron_permutation.pdf}
    \caption{\small \textbf{The \method\ Disagreement Test:} (taken from our experiment where $\Pc\ =\ $CIFAR 10 and $\Qc\ =\ $CIFAR 10.1 and sample size $n=50$) We start by
    training an ensemble of CDCs (we use and ensemble size of $5$) to disagree on a set of $n$ unseen samples $\Pun$ from the original training distribution
    while constrained to perform consistently with a base model on the original training and validation sets used to train the base model on CIFAR 10.
    We perform 100 of these calibration runs using different random seeds and samples for $\Pun$ to estimate a threshold $\tau$ such that $95\%$
        of the runs disagree on fewer than $\tau$ samples --- thereby fixing the significance level of the test to $5\%$.
        To estimate the test power, we train CDCs using \textbf{the exact same configuration} as the calibration runs except we replace $\Pun$ with a random set of $n$ samples $\boldQ$ from $\Qc$ (CIFAR 10.1).
        By averaging the number of runs that disagree on more than $\tau$ samples from $\boldQ$ we can compute the power (or true positive rate) of the test as $83\%$.}
    \label{fig:detectron_permutation}
\end{figure}

\subsection{Detectron Entropy}
We consider an additional variant, \textit{\method\ Entropy} (Det-E), which computes the mean prediction entropy of the CDC instead of relying solely on disagreement rates.
The intuition for Det-E draws from the fact that when CDCs satisfy their objective (i.e., in the case of harmful shift) they
learn to predict with high entropy on $\boldQ$ and low entropy on $\Pun$, resulting in a natural way to distinguish between distributions.
The CDC entropy is computed for a sample $x$ from the mean probabilities over each $N$ classes of the base classifier $f$ and set of CDCs $\mathbf{g}$.
\begin{equation}
    \text{CDC}_{\text{entropy}}(x)=-\sum_{c=1}^N {\hat{p}_c\log(\hat{p}_c)}\ \text{ where }\ \hat{p}_c \coloneqq \frac{1}{|\mathbf{g}|+1}\left(f(x)_c + \sum_{g\in \mathbf{g}} g(x)_c \right)
    \label{eq:cdc_entropy_def}
\end{equation}
We use a KS test to compute a $p$-value for covariate shift directly on the entropy distributions computed for $\boldQ$ and $\Pun$ and guarantee significance using the same strategy as Det-D
(This process is explained visually in \autoref{fig:entropy_test}).
The intuition for \textit{\method\ (Entropy)} draws from the fact that when CDCs satisfy their objective (i.e., in the case of harmful shift) they
learn to predict with high entropy on $\boldQ$ and low entropy on $\Pun$, resulting in a natural way to distinguish between distributions.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{images/entropy_test.pdf}
    \caption{\small \textbf{The \method\ Entropy Test:} Following the same experimental setup as \autoref{fig:detectron_permutation},
        we start (left) by computing a KS test between the empirical distributions entropy values for each calibration run $\Pun$ with the flattened set of entropy values from all other calibration runs.
        Then (center) we compute a KS test from each test run $\boldQ$ with a random set of all but one calibration runs.
        Finally (right), we find a threshold $\tau$ on the distribution of $p$-values obtained from step 1 as the lower $\alpha$ quantile to guarantee a false positive rate of $\alpha$.
        The power of the test is computed as the fraction of $p$-values computed from the test runs on $\boldQ$ that are below $\tau$.}
    \label{fig:entropy_test}
\end{figure}

\clearpage
\subsection{The Detectron Algorithm}
Concluding our methodology, we present a formal algorithmic description of the Detectron Disagreement test below in \autoref{alg:detectron}.
We release our code as open source software under a GNU General Public License v3.0.
\vspace{-5mm}
\begin{center}
    \href{https://github.com/tomginsberg/deeptst}{\includegraphics[width=.4\linewidth]{images/logo.pdf}}\\
    \vspace{-8mm}
    \url{https://github.com/tomginsberg/deeptst}
\end{center}

\begin{algorithm}[!htbt]
    \DontPrintSemicolon
    \SetAlgoLined
    \SetNoFillComment
    \SetKwRepeat{Do}{do}{while}%
    \caption{The Detectron algorithm for detecting harmful covariate shift}\label{alg:detectron}
    \KwIn{$\boldP$: labeled dataset, $\boldQ$: unlabeled dataset, ${L}$: learning algorithm,\\
        $K$: calibration rounds $=100$, $\aleph$: ensemble size $=5$, $\alpha$: significance level $=0.05$}
    \KwOut{test result for harmful covariate shift at significance level $\alpha$}
    \BlankLine
    $\boldP_{\text{train}},\ \boldP_{\text{val}},\ \Pun\ \gets\ \text{Partition}(\boldP)$

    $m\ \gets\ |\boldQ|$;\ $\Phi_{\Pun} \gets [\ ]$

    $f\ \gets\ {L}(\boldP_{\text{train}},\ \boldP_{\text{val}})$\ \tcp*{ Load or train a base classifier on $\boldP$}

    \Repeat{K iterations elapse}{
        $\mathbf{p}^\star\ \gets\ \text{ RandomSample}(\Pun, m)$

        \tcp{ Train an ensemble of CDCs on $\Pun$}
        \While{$|\mathbf{p}^\star|>0$ and iterations $\leq \aleph$}{
            $g\ \gets\ \text{ConstrainedDisagreement}({L}, \boldP_{\text{train}}, \boldP_{\text{val}}, \mathbf{p}^\star, f)$ \tcp*{See \autoref{alg:constrained}}

            $\mathbf{p}^\star \ \gets\ \{x\ |\ x\in \mathbf{p}^\star\text{ and } f(x)=g(x)\}$\tcp*{Filter out disagreed on data}

            $\phi_\Pun \ \gets 1-|\mathbf{p}^\star|/m$
            \tcp*{Update disagreement rate}}

        $\text{Append}\ \phi_{\boldP}\text{ to }\Phi_\boldP$}
    \tcp{ Train an ensemble of CDCs on $\boldQ$}
    \While{$\boldQ>0$ and iterations $\leq \aleph$}{
        $g\ \gets\ \text{ConstrainedDisagreement}({L}, \boldP_{\text{train}}, \boldP_{\text{val}}, \boldQ, f)$

        $\boldQ\ \gets\ \{x\ |\ x\in \boldQ\text{ and } f(x)=g(x)\}$

        $\phi_\boldQ \ \gets 1-|\boldQ|/m$
    }
    \Return{$\phi_\boldQ > \left[{(1-\alpha)}\text{ quantile of }\Phi_\Pun \right]$}
\end{algorithm}